###  分词器

一个analyzer即分析器，把一句话分成若干个单词，无论是内置的还是自定义的，只是一个包含character filters（字符过滤器）、 tokenizers（分词器）、token filters（令牌过滤器）三个细分模块的包

- 字符过滤器(CharacterFilters)：首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 & 转化成 and
- 分词器(Tokenizer)：字符串被 分词器 分为单个的词条。得到分词，标记每个分词的顺序或位置（用于邻近查询），标记分词的起始和结束的偏移量（用于突出显示搜索片段），标记分词的类型；
- 后过滤器(TokenFilter)：最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）



#### 1.字符过滤器(CharacterFilters)

-  使用html_strip字符过滤器

   tokenizer = keyword,表示使用keyword，不对输入的数据进行分词

  char_filter 表示 使用 “html_strip” 对数据进行过滤处理

  ```json
  GET _analyze
  {
      "tokenizer": "keyword",
      "char_filter": ["html_strip"],
      "text": ["<p>i am  &apos; so happy </p>"]
  }
  
  ```

  

  html_strip 的参数  “escaped_tags”  表示不进行过滤的标签名，多个标签用数组表示

  ```json
  PUT  my_index_001
  {
    "settings": {
      "analysis": {
        "analyzer": {
          "my_analyzer":{
            "tokenizer":"keyword",
            "char_filter":["my_html_strip"]
          }
        },
        "char_filter": {
           "my_html_strip":{
             "type":"html_strip",
             "escaped_tags":["b"]
           }
        }
      }
    }
  }
  
  
  GET my_index_001/_analyze
  {
      "tokenizer": "keyword",
     "char_filter": ["html_strip"],
      "text": ["<p>i am <b>好</b>  &apos; so happy </p>"]
  }
  
  输出结果  "i am <b>好</b>  ' so happy "
  ```

  

- Mapping字符过滤器

​    对文本进行替换

| 参数          | 说明                                                         |
| ------------- | ------------------------------------------------------------ |
| mappings      | 使用key => value来指定映射关系，多种映射关系用数组表示       |
| mappings_path | 指定配置了mappings映射关系的文件的路径，文件使用UTF-8格式编码，每个映射关系使用换行符分割 |

```json


GET /_analyze
{
  "tokenizer": "keyword",
  "char_filter": [
    {
      "type": "mapping",
      "mappings": [
        "你 => 他"
      ]
    }
  ],
  "text": "你好吗"
}

##  得到结果 [ 他好吗 ]

PUT /my-index-001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "char_filter": [
            "my_mappings_char_filter"
          ]
        }
      },
      "char_filter": {
        "my_mappings_char_filter": {
          "type": "mapping",
          "mappings": [
            ":) => _happy_",
            ":( => _sad_"
          ]
        }
      }
    }
  }
}

GET /my-index-000001/_analyze
{
  "tokenizer": "keyword",
  "char_filter": [ "my_mappings_char_filter" ],
  "text": "I'm delighted about it :)"
}

```



- 使用pattern_replace 字符过滤器

   对文本进行正则匹配，对匹配的字符串进行替换。

#### 2.分词器(Tokenizer)

- Standard  tokenizer是基于<Unicode标准附录#29>中指定的算法进行切分的，如whitespace，‘-’等符号都会进行切分。

  | 参数             | 说明                                                         | 默认值  |
  | ---------------- | ------------------------------------------------------------ | ------- |
  | max_token_length | 切分后得到的token的长度如果超过最大token长度，以最大长度间隔拆分 | 默认255 |

```json
POST _analyze
{
  "tokenizer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

# [ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]
   
 PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "standard",
          "max_token_length": 5
        }
      }
    }
  }
}

GET my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

# [ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]   jumped被拆分成了  jumpe  和 d

```



- Whitespace tokenizer 将文本通过空格进行分词。

  | 参数               | 说明                                 | 默认值   |
  | ------------------ | ------------------------------------ | -------- |
  | `max_token_length` | 经过此分词器后所得的数据的最大长度。 | 默认 255 |

```json
GET _analyze
{
  "tokenizer": "whitespace",
  "text": "我爱 中国"
}
会被分成 '我爱' 和 '中国'
```

- Lowercase 把分词结果全部换成小写格式
- Keyword Tokenizer 不会对文本进行操作，会将一整块的输入数据作为一个token。

####  3.后过滤器(TokenFilter)



-  [Synonym graph](https://www.elastic.co/guide/en/elasticsearch/reference/7.6/analysis-synonym-graph-tokenfilter.html)

    同义词

在 es 的config 目录下添加  synonym.txt   

```txt
i大G,da G => 奔驰G级
小小酥 => 旺旺
lol => 英雄联盟
```

```json

PUT my-index-000001
{
  "settings": {
    "analysis": {
      "filter": {
        "my_synonym": {
          "type": "synonym_graph",
          "synonyms_path": "synonym.txt"
        }
      },
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "ik_max_word",
          "filter": [
            "my_synonym"
          ]
        }
      }
    }
  }
}


GET my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": ["lol"]
}

# 会搜索出来  英雄联盟
```



- Synonym  

  ```json
  PUT my-index-000001
  {
    "settings": {
      "analysis": {
        "filter": {
          "my_synonym": {
            "type": "synonym",
            "synonyms": [
               "lol => 英雄联盟"
              ]
          }
        },
        "analyzer": {
          "my_analyzer": {
            "tokenizer": "ik_max_word",
            "filter": [
              "my_synonym"
            ]
          }
        }
      }
    }
  }
  
  
  
  GET my-index-000001/_analyze
  {
    "analyzer": "my_analyzer",
    "text": ["lol"]
  }
  ```

  



-  大小写  lowercase、uppercase

  ```json
  
  GET _analyze
  {
    "tokenizer" : "standard",
    "filter" : ["lowercase"],
    "text" : "THE Quick FoX JUMPs"
  }
  
  
  
  ```

  

- stop 停用词 ,从令牌流中删除停用词 

```json

PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "ik_max_word",
          "filter": [ "my_custom_stop_words_filter" ]
        }
      },
      "filter": {
        "my_custom_stop_words_filter": {
          "type": "stop",
          "ignore_case": true,
          "stopwords": [ "的", "吗", "是" ]
        }
      }
    }
  }
}

GET my-index-000001/_analyze
{
  "tokenizer" : "ik_max_word",
  "filter" : ["my_custom_stop_words_filter"],
  "text" : "我的老师是一个好人吗"
}
###  结果中不会出现  的 吗 是 

GET _analyze
{
  "tokenizer" : "ik_max_word",
  "text" : "我的老师是一个好人吗"
}
###  结果中会出现  的 吗 是 
```









### 自定义分词器

```json
PUT   custom_analysis
{
  "settings": {
    "analysis": {
      "char_filter": {
        "my_char_filter":{
          "type":"mapping",
          "mappings":[
            "& => and",
            "| => or"
            ]
        },
        "html_strip_cahr_filter":{
          "type":"html_strip",
          "escaped_tags":["a"]
        }
      },
      "filter": {
        "my_stopword":{
          "type":"stop",
          "stopwords":[
            "is",
            "in",
            "the","a","at","for"
            ]
        }
      },
      "tokenizer": {
        "my_tokenizer":{
          "type":"pattern",
          "pattern":"[ ,.、?!]"
        }
      }, 
      "analyzer": {
        "my_analyzer":{
          "type":"custom",
          "char_filter":["my_char_filter","html_strip_cahr_filter"],
           "filter":["my_stopword","lowercase"]
           ,

          "tokenizer":"my_tokenizer"
        }
      }
    }
  }
}


GET custom_analysis/_analyze
{
  "analyzer": "my_analyzer",
  "text":["WHat is, adads .in <b> 喝喝 </b>  & | is in at for "]
}

分词结果： [what,adads,喝喝,and,or]
```



char_filter 指定字符过滤器，tokenizer指定分词逻辑，filter 指定分词后的处理逻辑

